---
title: "p8105_hw6_hz2771"
author: 'Haolin Zhong (UNI: hz2771)'
date: "2021/12/3"
output: github_document
---

### Import dependencies

```{r, message=FALSE}
library(tidyverse)
library(modelr)
library(patchwork)
```


## Problem 1

### Load and clean data

Here we load and clean the data. In the data cleaning process, we transformed
`babysex`, `frace`, `mrace`, `malform` into factors. In addition, we also 
ascertained that there is no missing data.

```{r, warning=FALSE, message=FALSE}
data_path = "./data/birthweight.csv"

bwt = read_csv(data_path) %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(frace,
                       "white" = "1", 
                       "black" = "2", 
                       "asian" = "3", 
                       "puerto rican" = "4",
                       "other" = "8",
                       "unknown" = "9"),
    mrace = as.factor(mrace),
    mrace = fct_recode(mrace,
                       "white" = "1", 
                       "black" = "2", 
                       "asian" = "3", 
                       "puerto rican" = "4",
                       "other" = "8"),
    malform = as.factor(malform),
    malform = fct_recode(malform,
                         "absent" = "0",
                         "present" = "1")
    
  )

skimr::skim(bwt)[c(2,4)] %>% t() %>% knitr::kable()
```

### Build models

At first, we build one model of our own. Intuitively, we consider several 
variables, including `babysex`, `bhead`, `blength`, `wtgain`, `ppwt`, `pnumlbw`,
may be strong predictors of infant birth weight. Therefore, we build
models using every subsets of these variables, and choose the best model which
has the smallest BIC value.


```{r}
# find all possible subsets of variables
vset = unlist(lapply(1:6, 
                    combn, 
                    x = c("babysex", "bhead", "blength", "wtgain", "ppwt", "pnumlbw"), 
                    simplify = FALSE), 
              recursive = FALSE)

calc_BIC = function(variables){
  formula = as.formula(paste("bwt", paste(variables, collapse = " + "), sep = "~"))
  model = lm(formula, bwt)
  return(broom::glance(model) %>% pull("BIC"))
}

# calculate BIC value of every model
BICs = map(vset, calc_BIC) %>% as_vector()

# choose model with the lowest BIC
index = which(BICs == min(BICs))[[1]]
variables = vset[[index]]
formula = as.formula(paste("bwt", paste(variables, collapse = " + "), sep = "~"))
fit_own = lm(formula, bwt)

bwt %>% 
  add_residuals(fit_own) %>% 
  add_predictions(fit_own) %>% 
  ggplot(aes(x = pred, y =resid)) +
  geom_point() +
  labs(
    x = "fitted values",
    y = "residuals",
    title = "model residuals against fitted values"
  )
```

Finally, our model used `babysex`, `bhead`, `blength`, `wtgain`, `ppwt` as 
features.


Then, we build two other models, one using length at birth and gestational age as
predictors, one using head circumference, length, sex, and all interactions 
between these and compare the previous model with the two models. We then 
compare the three models' cross-validated prediction error, i.e. RMSE.


```{r}
crossv_mc(bwt, 100) %>% 
  mutate(
    fit_own = map(train, ~lm(bwt ~ babysex + bhead + blength + wtgain + ppwt, data = .x)),
    fit_simple = map(train, ~lm(bwt ~ blength + gaweeks, bwt, data = .x)),
    fit_complex = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))
  ) %>% 
  mutate(
    error_fit_own = map2_dbl(fit_own, test, rmse),
    error_fit_simple = map2_dbl(fit_simple, test, rmse),
    error_fit_complex = map2_dbl(fit_complex, test, rmse)
  ) %>% 
  select("error_fit_own":"error_fit_complex") %>% 
  pivot_longer(names_to = "model", names_prefix = "error_", values_to = "rmse",
               cols = everything()) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(
    y = "RMSE",
    title = "Cross validated RMSE among models"
  )
```

We found that our own model has the lowest cross-validated RMSE, while the 
simple model has the largest cross-validated RMSE.


## Problem 2

### Download data

```{r, message=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

### bootstrap


Here we plot the distribution for $\hat{r}^2$ and $\text{log}(\widehat{\beta}_0 * \widehat{\beta}_1)$.

```{r}
weather_bs = modelr::bootstrap(weather_df, n = 5000)

r_square_df = 
  weather_bs %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::glance)
  ) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(r.squared) 
  
beta_df = 
  weather_bs %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(-models, -strap) %>% 
  unnest(results) %>% 
  select(`.id`, term, estimate) %>% 
  pivot_wider(names_from = term, values_from = estimate) %>% 
  janitor::clean_names() %>% 
  mutate(logbeta = log(intercept * tmin)) %>% 
  select(logbeta)

r_sqr_distribution = 
  r_square_df %>% 
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
    title = expression(paste("Distribution of ", hat(r)^2)),
    x = expression(hat(r)^2)
  )

logbeta_distribution = 
  beta_df %>% 
  ggplot(aes(x = logbeta)) +
  geom_density() +
  labs(
    title = expression(paste("Distribution of ", log(hat(beta[0]) * hat(beta[1])))),
    x = expression(log(hat(beta[0]) * hat(beta[1])))
  )

r_sqr_distribution / logbeta_distribution
```

We can observe that the two distributions are both close to normal distribution, 
yet the $\hat{r}^2$ distribution is slightly left-skewed and has some outliers.

In addition, we can calculate the 95% CI for the two terms:

```{r}
r_sqr_ci =
  r_square_df %>% 
  summarise(
    ci_lower = quantile(r.squared, 0.025), 
    ci_upper = quantile(r.squared, 0.975)
    )

logbeta_ci = 
  beta_df %>% 
  summarise(
    ci_lower = quantile(logbeta, 0.025), 
    ci_upper = quantile(logbeta, 0.975)
    )

bind_rows(r_square = r_sqr_ci, log_b0b1 = logbeta_ci, .id = "term") %>% 
  knitr::kable(digits = 3)
```

